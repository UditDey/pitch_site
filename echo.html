<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Inference Optimization Proposals</title>
    <link rel="stylesheet" href="style.css">
    <script defer src="common.js"></script>
</head>

<body>
    <header>
        <div class="container">
            <a href="index.html" class="back-link">← Back to Home</a>
        </div>
    </header>

    <main>
        <div class="container">
            <h1>Echo Cache for Codebook Decompression of LLM Weights</h1>
            <p class="subtitle">A multiported, scalable hardware cache for wide gather operations</p>

            <h2>The Problem</h2>
            <p>
                As discussed in <a href="prereq.html#codebook_compression">Prerequisite Concepts</a>, inference on
                codebook compressed 1LLMs requires reconstructing (i.e. decompressing) the weight matrices like so:
            <pre><code class="language-python">for idx in indices:
    weight_group = codebook[idx]</code></pre>

            And the <code>codebook[idx]</code> operation makes this an indirect memory access pattern.
            </p>

            <p>
                This makes things particularly worse for vector processors. If we try to parallelize
                this decompression on a vector processor, each thread could end up with a different <code>idx</code>.
                This is a <u>gather</u> operation and read-coalescion is impossible.
            </p>
            <figure>
                <img src="img/echo/gather.svg" title="Example Gather Operation" alt="Example Gather Operation">
                <figcaption>
                    FIgure 1: Example gather operation. There are four readers who's addresses need to be dereferenced
                    in parallel. Note that two of the readers here issue the same address and the data needs to be
                    "multicasted".
                </figcaption>
            </figure>
            <p>
                This means that despite significant reductions in weight size, codebook compression methods can be
                slower than their uncompressed counterparts when implemented with regular CUDA kernels. Since the rest
                of the <code>matmul</code> procedure is the same, it points to the <u>gather-op bottlenecking the entire
                    system</u>.
            </p>
            <p>
                For the rest of this article I will assume that the codebook itself is small enough to fit in an on-chip
                cache. Practically, this assumption holds true. A large codebook will have more bits-per-index too,
                which defeats the point of compression. This <u>codebook cache</u> is the main focus of this article.
            </p>

            <h2>Multiporting and Cache Banking</h2>
            <p>
                The speed at which gather-ops can be served clearly depends on the number of read-ports of the cache
                (i.e. a multiported cache). Here, we need to clarify two types of multiporting:
            <ul>
                <li>True Multiporting</li>
                <li>Virtual Multiporting</li>
            </ul>
            </p>
            <p>
                For true-multiporting, think of multiported SRAMs. There are multiple usable read-ports, no caveats.
                This would be ideal, but is also the hardest. If we have hundreds of parallel threads, ideally we need
                hundreds of true-read-ports. <u>A hundred read-port SRAM is not feasible.</u>
            </p>
            <p>
                By virtual-multiporting (or VMP), I mean any system that builds a large number from read-ports using
                caches with a smaller number of read-ports.
            </p>
            <p>
                <u>Banking</u> (or slicing) is the most common VMP solution. But it introduces two problems:
            <ol>
                <li>How are the banks' true-ports connected to the virtual-ports? What is the interconnect complexity
                    when the number of banks and virtual-ports are very high?</li>
                <li>If many ports want to access the same bank, the requests must be serialized, significantly reducing
                    throughput.</li>
            </ol>
            </p>
            <p>
                The design I present in this article builds on standard cache banking, but introduces a mitigation for
                these two problems.
            </p>

            <h2>Interconnection Network</h2>
            <p>
                Following what was introduced in <a href="prereq.html#mins">Prerequisite Concepts</a>, we could consider
                a crossbar to solve problem 1. But the obstacle is the complexity of the crossbar when we have
                potentially hundreds of virtual read-ports. A crossbar could be completely infeasible due to <u>long
                    wires</u> and <u>extreme routing congestion</u>.
            </p>
            <p>
                We should instead consider Multistage Interconnection Networks (MINs). MINs have $O(log \ N)$
                complexity,
                as opposed to crossbars' $O(N^2)$, making it more suitable for hundreds of read-ports.
            </p>
            <p>
                However, MINs bring their own set of complexities, mainly that <u>figuring out the switch settings</u>
                to achieve a desired routing in a MIN is a <u>non-trivial task</u>. Some MINs (like Beneš Networks) are
                very powerful, but require complex routing algorithms to calculate switch settings, making them
                impractical for our purpose.
            </p>
            <p>
                Instead, we will use a class of MINs, known as <u>self-routing MINs</u>, to create the interconnect. In
                a self-routing MIN, the data packets contain their destination address, and each switch looks at a bit
                from the address to make its own, entirely <u>local routing decision</u>. No global coordination or
                complex routing algorithm needed. We simply <u>inject packets</u> at the input, and they <u>find their
                    own way</u> through the network.
            </p>
            <figure>
                <img src="img/echo/example_min.svg" title="Diagram of a Multistage Interconnect Network"
                    alt="Diagram of a Multistage Interconnect Network">
                <figcaption>
                    Figure 1: General diagram of a Multistage Interconnection Network. There is a grid of switches (the
                    opaque boxes) connected via some wiring topology (the dashed boxes), and input/output ports on each
                    side.
                </figcaption>
            </figure>
            <div class="callout aside">
                <strong>Resources</strong>
                <p>
                    MINs can be complex to learn and this article is <i>not</i> the right place to learn about them for
                    the first time. I will again emphasize that readers should read the <a
                        href="prereq.html#mins">Prerequisite Concepts</a> page, which also includes a list of resources
                    to learn about MINs.
                </p>
            </div>

            <h2>The Echo Cache</h2>
            <p>
                This is my solution:
            <ul>
                <li>Use a self-routing MIN as the interconnect between cache-banks and virtual read-ports</li>
                <li>In phase 1, make addresses travel across the interconnect, from virtual ports to banks</li>
                <li>In phase 2, make data travel from banks to virtual ports, tracing the same route the address packets
                    took in phase 1, except in reverse</li>
            </ul>
            </p>
            <p>
                Hence the name, <b>Echo Cache</b>. Addresses travel to banks, data returns back like an echo. And since
                a MIN is used instead of a crossbar, it can be feasibly scaled to hundreds of virtual read-ports with
                feasible wire lengths and routing congestion.
            </p>
            <p>
                In addition, the switches in the MIN have to be given a "packet merging" capability. This allows the
                banks to do <u>fan-out data transport</u>, i.e. <u>broadcast</u>. This significantly reduces the amount
                of serialization that takes place, since if two read-ports want the same address, the data can be
                replicated in the MIN, and broadcast to all requesting ports. A significant number of read-ports can
                receive their data in parallel.
            </p>
            <p>
                Hence, both problems mentioned in the previous section have been addressed.
            </p>

            <h2>Detailed Explanation</h2>
            <figure>
                <img src="img/echo/dual_omega_storage.svg" title="Echo Cache Diagram" alt="Echo Cache Diagram">
                <figcaption>
                    Figure 1: Overall Architecture of the Echo Cache. Addresses flow down the control network, data
                    flows up the transfer network.
                </figcaption>
            </figure>
            <p>
                It's best to understand the design as a pair of MINs: the <b>control network</b> and the <b>transfer
                    network</b>. Both are self-routing, and switches make their own local decisions. The control network
                deals with <u>address packets</u> and the transfer network deals with <u>data packets</u>.
            </p>
            <p>
                First, address packets are injected from the read-ports and make their way through the self-routing
                control network. Then, switch states are copied to the transfer network, and data packets flows from
                banks back to read-ports, following the same paths in <i>reverse</i>.
            </p>
            <p>
                The control network essentially only performs a "dry-run", that the transfer network replays in reverse
                to transport data packets. This retains the advantage of self-routing MINs: no complex routing algorithm
                needs to be used to compute switch states. The control network <i>is</i> the algorithm.
            </p>
            <p>
                Now, most self-routing MINs have chances of packet collisions or <u>blocking</u>, i.e. when two packets
                at a switch input want to both go to the same output wire. In this case, one packet must be
                <u>dropped</u>. However,if both packets are ultimately going to the same bank, then instead of dropping,
                both packets can be <u>merged</u>.
            </p>
            <p>
                On the way down, packets merge and form a fan-in routing. When reversed, packets <u>replicate</u> and
                form a fan-out routing. This is the core mechanism behind the fan-out/broadcast capability of the Echo
                Cache.
            </p>
            <figure>
                <img src="img/echo/fanin_fanout.svg" title="Fan-in to Fan-out Diagram" alt="Fan-in to Fan-out Diagram">
                <figcaption>
                    Figure 2: Visual example of a fan-in routing becoming a fan-out routing when traced in reverse. This
                    is what allows the storage-array to "multicast" data to read-ports, reducing serialization.
                </figcaption>
            </figure>

            <p>
                This presents an issue: if some packets are dropped in the control phase, those read ports will never
                receive their data. To solve this, we include <u>retry logic</u>, that sees which read ports finally
                receive valid data, and selectively re-issues the rest back into the control network.
            </p>
            <p>
                This is a form of serialization (hence why I mentioned that serialization is reduced, and not
                eliminated). However, the probability of blocking depends on the MIN topology chosen. If we use designs
                like <u>dilated networks</u> or <u>asymptotically nonblocking networks</u>, probability of blocking is
                reduced, and we have to serialize rarely, maintaining high system throughput.
            </p>

            <div class="callout aside">
                <strong>Are 2 Networks Necessary?</strong>
                <p>
                    No. The control and transfer phases can technically be done on a single physical network, provided
                    that switches are truly bidirectional, which would require <code>inout</code> wires and
                    high-z/tri-state logic.
                </p>
                <p>
                    In fact, more than 2 networks can also be used. We can use multiple transfer networks to divide the
                    workload of carrying wide-bitwidth data. This can potentially reduce wire congestion.
                </p>
            </div>

            <div class="callout aside">
                <strong>What About the Write-Ports?</strong>
                <p>
                    This is designed for gather, not scatter. So we assume that the number of write ports needed is
                    small, and a regular bus/crossbar connection for the write side of things is feasible. Its only for
                    the hundreds of read-ports that we need MINs.
                </p>
                <p>
                    In LLM codebook decompression, this tracks. We dereference indices <i>far</i> more frequently than
                    we load codebooks.
                </p>
            </div>

            <p>
                Lets take the example of a MIN topology that uses a grid of 2x2 switches. The following diagrams explain
                the different states each switch can be in:
            </p>
            <figure>
                <img src="img/echo/switch_states.svg" title="Example States of 2x2 Switch"
                    alt="Example States of 2x2 Switch" style="width: 50%;">
                <figcaption>
                    Figure 3: The 4 basic states of a 2x2 switch
                </figcaption>
            </figure>
            <figure>
                <img src="img/echo/switch_conflict.svg" title="Example Blocking States of a 2x2 Switch"
                    alt="Example Blocking States of a 2x2 Switch" style="width: 50%;">
                <figcaption>
                    Figure 4: The 2 additional packet dropping states when there is contention but
                    packets cannot be merged. We have a static rule here: the left input packet always loses.
                </figcaption>
            </figure>
            <figure>
                <img src="img/echo/switch_bidir.svg" title="Bidirectional Switch Behaviour"
                    alt="Bidirectional Switch Behaviour" style="width: 90%;">
                <figcaption>
                    Figure 5: Interpretation of switch behaviour in forward vs reverse directions. Note that the switch
                    becomes asymmetrical when blocking or merging takes place.
                </figcaption>
            </figure>
            <p>
                Figure 5 demonstrates switch behaviour in both directions. For example, let's say a switch in the
                control network decides on the "left-merge" state, i.e. it merges two address packets and forwards them
                to the left output. Then in the transfer network, the corresponding switch will operate in
                <i>reverse</i>, i.e. it will duplicate a data packet, enabling multicast behaviour.
            </p>

            <h2>Verilog Sample</h2>
            <p>
                I developed a proof-of-concept RTL implementation of the Echo Cache which <a style="white-space: nowrap"
                    href="https://github.com/UditDey/pitch_site/blob/main/code/echo">can be
                    found here</a>. It utilizes $2 \times 2$ switches with the <a
                    href="https://en.wikipedia.org/wiki/Omega_network">omega network</a> topology, the
                simplest self-routing MIN.
            </p>
            <p>
                Here, I will present the design of just the switch modules, which should be enough to get a grasp of how
                the whole system operates. <code>ctrl_switch</code> routes its input packets, and registers its
                decision. <code>xfer_switch</code> reads the saved the decision, and performs the same operation in
                reverse.
            </p>

            <pre><code class="language-verilog">// ****************** Packet Layouts ******************
// Address Packet: {valid, addr}                      *
// Data Packet:    {valid, data}                      *
//                                                    *
// RULE: When valid = 0, data/addr = 0 as well        *
// ****************************************************



// ****************** Control Switch ******************
// Input Packets:  in0,  in1                          *
// Output Packets: out0, out1                         *
//                                                    *
// Routing decision stored in: did_swap, did_merge:   *
// 1) (0, 0): Passthru                                *
// 2) (1, 0): Swap                                    *
// 3) (0, 1): Merge left                              *
// 4) (1, 1): Merge Right                             *
// ****************************************************
module ctrl_switch #(
    parameter ADDR_WIDTH = 3,
    parameter STAGE_BIT  = 0
) (
    input  wire clk,
    input  wire rst,
    input  wire [ADDR_WIDTH:0] in0,
    input  wire [ADDR_WIDTH:0] in1,
    output reg  [ADDR_WIDTH:0] out0,
    output reg  [ADDR_WIDTH:0] out1,
    output reg                 did_swap,
    output reg                 did_merge
);    
    localparam PKT_WIDTH = ADDR_WIDTH + 1;
    localparam [PKT_WIDTH-1:0] ZERO_PKT = {PKT_WIDTH{1'b0}};

    // Read packets
    wire in0_valid       = in0[PKT_WIDTH-1];
    wire in0_wants_right = in0[STAGE_BIT];
    wire in1_valid       = in1[PKT_WIDTH-1];
    wire in1_wants_right = in1[STAGE_BIT];

    // Switch decision
    wire do_swap  = in0_valid ? in0_wants_right :
                    in1_valid ? ~in1_wants_right :
                    1'b0;

    wire do_merge = in0_valid && in1_valid && (in0[ADDR_WIDTH-1:0] == in1[ADDR_WIDTH-1:0]);

    // Output crossbar
    wire [ADDR_WIDTH:0] xbar0 = do_swap ? in1 : in0;
    wire [ADDR_WIDTH:0] xbar1 = do_swap ? in0 : in1;

    // Kill misrouted packet
    wire kill0 = xbar0[STAGE_BIT] == 1'b1;
    wire kill1 = xbar1[STAGE_BIT] == 1'b0;

    // Final ouput packets
    wire [ADDR_WIDTH:0] final_out0 = kill0 ? ZERO_PKT : xbar0;
    wire [ADDR_WIDTH:0] final_out1 = kill1 ? ZERO_PKT : xbar1;

    // Register outputs
    always @(posedge clk) begin
        if (rst) begin
            out0      <= ZERO_PKT;
            out1      <= ZERO_PKT;
            did_swap  <= 1'b0;
            did_merge <= 1'b0;
        end else begin
            out0      <= final_out0;
            out1      <= final_out1;
            did_swap  <= do_swap;
            did_merge <= do_merge;
        end
    end
endmodule


// ****************** Transfer Switch ******************
// Input Packets:  in0,  in1                           *
// Output Packets: out0, out1                          *
//                                                     *
// Switch takes decision from (did_swap, did_merge)    *
// inputs, and follows them in reverse                 *
// *****************************************************
module xfer_switch #(
    parameter DATA_WIDTH = 8,
    parameter STAGE_BIT  = 0
) (
    input  wire clk,
    input  wire rst,
    input  wire [DATA_WIDTH:0] in0,
    input  wire [DATA_WIDTH:0] in1,
    input  wire                did_swap,
    input  wire                did_merge,
    output reg  [DATA_WIDTH:0] out0,
    output reg  [DATA_WIDTH:0] out1
);
    localparam PKT_WIDTH = DATA_WIDTH + 1;
    localparam [PKT_WIDTH-1:0] ZERO_PKT = {PKT_WIDTH{1'b0}};

    // Reverse the crossbar using stored swap decision
    wire [DATA_WIDTH:0] xbar0 = did_swap ? in1 : in0;
    wire [DATA_WIDTH:0] xbar1 = did_swap ? in0 : in1;
    
    // Merge case: Choose the valid packet to multicast
    wire [DATA_WIDTH:0] merge_chosen = in0 | in1;
    wire [DATA_WIDTH:0] final_out0 = did_merge ? merge_chosen : xbar0;
    wire [DATA_WIDTH:0] final_out1 = did_merge ? merge_chosen : xbar1;

    // Register outputs
    always @(posedge clk) begin
        if (rst) begin
            out0 <= ZERO_PKT;
            out1 <= ZERO_PKT;
        end else begin
            out0 <= final_out0;
            out1 <= final_out1;
        end
    end
endmodule</code></pre>

            <br>
            <p>
                In the omega network topology, layers of switches are connected via the <u>perfect shuffle</u> wiring
                pattern. Accordingly, when tracing in reverse, the transfer network uses the <u>inverse perfect
                    shuffle</u>.
            </p>
            <pre><code class="language-verilog">module perfect_shuffle #(
    parameter N         = 8,
    parameter PKT_WIDTH = 4
) (
    input  wire [N*PKT_WIDTH-1:0] in_pkts,
    output wire [N*PKT_WIDTH-1:0] out_pkts
);
    localparam LOG_N = $clog2(N);

    genvar i;
    generate
        for (i = 0; i < N; i = i + 1) begin : shuffle
            localparam integer j = ((i << 1) & (N - 1)) | (i >> (LOG_N - 1));

            assign out_pkts[j*PKT_WIDTH +: PKT_WIDTH] = in_pkts[i*PKT_WIDTH +: PKT_WIDTH];
        end
    endgenerate
endmodule


module inverse_perfect_shuffle #(
    parameter N         = 8,
    parameter PKT_WIDTH = 4
) (
    input  wire [N*PKT_WIDTH-1:0] in_pkts,
    output wire [N*PKT_WIDTH-1:0] out_pkts
);
    localparam LOG_N = $clog2(N);

    genvar i;
    generate
        for (i = 0; i < N; i = i + 1) begin : unshuffle
            localparam integer j = (i >> 1) | ((i & 1) << (LOG_N - 1));

            assign out_pkts[j*PKT_WIDTH +: PKT_WIDTH] = in_pkts[i*PKT_WIDTH +: PKT_WIDTH];
        end
    endgenerate
endmodule</code></pre>

            <br>
            <p>Finally, the network modules can be formed like so:</p>
            <pre><code class="language-verilog">module ctrl_layer #(
    parameter N          = 8,
    parameter ADDR_WIDTH = 3,
    parameter STAGE_BIT  = 0
) (
    input  wire clk,
    input  wire rst,
    input  wire [N*(ADDR_WIDTH+1)-1:0] in_pkts,
    output wire [N*(ADDR_WIDTH+1)-1:0] out_pkts,
    output wire [(N/2)-1:0]            did_swaps,
    output wire [(N/2)-1:0]            did_merges
);
    localparam PKT_WIDTH = ADDR_WIDTH + 1;

    genvar i;
    generate
        wire [N*PKT_WIDTH-1:0] sh_pkts;

        perfect_shuffle #(
            .N(N),
            .PKT_WIDTH(PKT_WIDTH)
        ) shuffle (
            .in_pkts(in_pkts),
            .out_pkts(sh_pkts)
        );

        for (i = 0; i < N/2; i = i + 1) begin : sw        
            ctrl_switch #(
                .ADDR_WIDTH(ADDR_WIDTH),
                .STAGE_BIT(STAGE_BIT)
            ) switch (
                .clk(clk),
                .rst(rst),
                .in0 (sh_pkts [(2*i  ) * PKT_WIDTH +: PKT_WIDTH]),
                .in1 (sh_pkts [(2*i+1) * PKT_WIDTH +: PKT_WIDTH]),
                .out0(out_pkts[(2*i  ) * PKT_WIDTH +: PKT_WIDTH]),
                .out1(out_pkts[(2*i+1) * PKT_WIDTH +: PKT_WIDTH]),
                .did_swap(did_swaps[i]),
                .did_merge(did_merges[i])
            );
        end
    endgenerate
endmodule


module xfer_layer #(
    parameter N          = 8,
    parameter DATA_WIDTH = 3,
    parameter STAGE_BIT  = 0
) (
    input  wire clk,
    input  wire rst,
    input  wire [N*(DATA_WIDTH+1)-1:0] in_pkts,
    input  wire [(N/2)-1:0]            did_swaps,
    input  wire [(N/2)-1:0]            did_merges,
    output wire [N*(DATA_WIDTH+1)-1:0] out_pkts
);
    localparam PKT_WIDTH = DATA_WIDTH + 1;

    genvar i;
    generate
        wire [N*PKT_WIDTH-1:0] tmp_pkts;

        for (i = 0; i < N/2; i = i + 1) begin : sw        
            xfer_switch #(
                .DATA_WIDTH(DATA_WIDTH),
                .STAGE_BIT(STAGE_BIT)
            ) switch (
                .clk(clk),
                .rst(rst),
                .in0 (in_pkts [(2*i  ) * PKT_WIDTH +: PKT_WIDTH]),
                .in1 (in_pkts [(2*i+1) * PKT_WIDTH +: PKT_WIDTH]),
                .did_swap(did_swaps[i]),
                .did_merge(did_merges[i]),
                .out0(tmp_pkts[(2*i  ) * PKT_WIDTH +: PKT_WIDTH]),
                .out1(tmp_pkts[(2*i+1) * PKT_WIDTH +: PKT_WIDTH])
            );
        end

        inverse_perfect_shuffle #(
            .N(N),
            .PKT_WIDTH(PKT_WIDTH)
        ) shuffle (
            .in_pkts(tmp_pkts),
            .out_pkts(out_pkts)
        );
    endgenerate
endmodule


module ctrl_network #(
    parameter N = 8
) (
    input  wire                   clk,
    input  wire                   rst,
    input  wire [N-1:0]           req_valids,
    input  wire [N*$clog2(N)-1:0] addrs,
    output wire [N*($clog2(N)+1)-1:0]     out_pkts,
    output wire [((N/2) * $clog2(N))-1:0] did_swaps,
    output wire [((N/2) * $clog2(N))-1:0] did_merges
);
    localparam ADDR_WIDTH = $clog2(N);
    localparam NUM_STAGES = $clog2(N);
    localparam PKT_WIDTH  = ADDR_WIDTH + 1;

    // Expand addresses into packets: {valid, addr}
    wire [N*PKT_WIDTH-1:0] in_pkts;
    genvar p;
    generate
        for (p = 0; p < N; p = p + 1) begin : expand
            // Use the external valid signal instead of hardcoded 1'b1
            assign in_pkts[p*PKT_WIDTH +: PKT_WIDTH] = 
                {req_valids[p], addrs[p*ADDR_WIDTH +: ADDR_WIDTH]};
        end
    endgenerate

    // Inter-stage packet wires
    wire [N*PKT_WIDTH-1:0] stage_pkts [0:NUM_STAGES];
    assign stage_pkts[0] = in_pkts;

    // Instantiate stages (MSB routed first)
    genvar s;
    generate
        for (s = 0; s < NUM_STAGES; s = s + 1) begin : stage
            ctrl_layer #(
                .N(N),
                .ADDR_WIDTH(ADDR_WIDTH),
                .STAGE_BIT(ADDR_WIDTH - 1 - s)
            ) layer (
                .clk(clk),
                .rst(rst),
                .in_pkts(stage_pkts[s]),
                .out_pkts(stage_pkts[s+1]),
                .did_swaps(did_swaps[s*(N/2) +: (N/2)]),
                .did_merges(did_merges[s*(N/2) +: (N/2)])
            );
        end
    endgenerate

    assign out_pkts = stage_pkts[NUM_STAGES];
endmodule


module xfer_network #(
    parameter N          = 8,
    parameter DATA_WIDTH = 16
) (
    input  wire                           clk,
    input  wire                           rst,
    input  wire [N*DATA_WIDTH-1:0]        tap_ports,
    input  wire [N*($clog2(N)+1)-1:0]     ctrl_out_pkts,
    input  wire [((N/2) * $clog2(N))-1:0] did_swaps,
    input  wire [((N/2) * $clog2(N))-1:0] did_merges,
    output wire [N*(DATA_WIDTH+1)-1:0]    out_pkts
);
    localparam ADDR_WIDTH  = $clog2(N);
    localparam NUM_STAGES  = $clog2(N);
    localparam CTRL_PKT_WIDTH = ADDR_WIDTH + 1;
    localparam PKT_WIDTH   = DATA_WIDTH + 1;

    // Expand tap_ports into packets, using ctrl_out valid bits
    wire [N*PKT_WIDTH-1:0] in_pkts;
    genvar p;
    generate
        for (p = 0; p < N; p = p + 1) begin : expand
            wire ctrl_valid = ctrl_out_pkts[(p+1)*CTRL_PKT_WIDTH - 1];
            assign in_pkts[p*PKT_WIDTH +: PKT_WIDTH] = 
                ctrl_valid ? {1'b1, tap_ports[p*DATA_WIDTH +: DATA_WIDTH]}
                           : {PKT_WIDTH{1'b0}};
        end
    endgenerate

    // Inter-stage packet wires
    wire [N*PKT_WIDTH-1:0] stage_pkts [0:NUM_STAGES];
    assign stage_pkts[0] = in_pkts;

    // Instantiate stages in reverse order
    genvar s;
    generate
        for (s = 0; s < NUM_STAGES; s = s + 1) begin : stage
            localparam CTRL_STAGE = NUM_STAGES - 1 - s;
            
            xfer_layer #(
                .N(N),
                .DATA_WIDTH(DATA_WIDTH),
                .STAGE_BIT(s)
            ) layer (
                .clk(clk),
                .rst(rst),
                .in_pkts(stage_pkts[s]),
                .out_pkts(stage_pkts[s+1]),
                .did_swaps(did_swaps[CTRL_STAGE*(N/2) +: (N/2)]),
                .did_merges(did_merges[CTRL_STAGE*(N/2) +: (N/2)])
            );
        end
    endgenerate

    assign out_pkts = stage_pkts[NUM_STAGES];
endmodule</code></pre>

            <h2>Conclusion</h2>
            <p>
                In this article I presented the <b>Echo Cache</b>: a banked, virtual-multiported cache that uses a novel
                2 phase MIN to route addresses to banks, and then data back to read-ports.
            </p>
            <p>
                Instead of typical crossbars, I propose using MINs, which have $O(log \ N)$ complexity, as
                opposed to crossbars' $O(N^2)$. The use of MINs allows for a very large number of virtual read-ports,
                without scalability issues.
            </p>
            <p>
                The motivations for this are the wide gather operations that we encounter in LLM codebook compression.
                Codebook compression is a powerful idea that has been unfortunately bottlenecked by existing cache
                designs, and I hope that next-gen AI inference hardware can solve this problem at a hardware level using
                approaches like the Echo Cache.
            </p>
            <p>
                My <a href="ccpo.html">next article</a> further builds on LLM codebook compression, showing that
                reduction in model quality incurred can be mitigated by "patching"-in salient, outlier weights.
            </p>
        </div>
    </main>

    <footer>
        <div class="container">
            <a href="index.html" class="back-link">← Back to Home</a>
            <p>
                <a href="mailto:you@example.com">Email</a> ·
                <a href="https://linkedin.com/in/you">LinkedIn</a>
            </p>
        </div>
    </footer>
</body>

</html>