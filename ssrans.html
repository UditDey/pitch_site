<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Inference Optimization Proposals</title>
    <link rel="stylesheet" href="style.css">
    <script defer src="common.js"></script>
</head>

<body>
    <header>
        <div class="container">
            <a href="index.html" class="back-link">← Back to Home</a>
        </div>
    </header>

    <main>
        <div class="container">
            <h1>Sparsity-Specialized rANS Compression</h1>
            <p class="subtitle">A fast, vector algorithm for compressing unstructured sparse data</p>

            <h2>The Problem</h2>
            <p>
                This article was inspired by <a href="https://arxiv.org/abs/2505.22913">MUSTAFAR</a>, a KV cache pruning
                technique that allows us to significantly reduce the memory
                footprint during LLM inference. By zeroing out a significant number of entries in the KV matrix, we
                <u>reduce its information content</u> or its <u>Shannon entropy</u>. The resulting low-entropy,
                <u>sparse matrix</u> can be compressed in memory
            </p>
            <p>
                MUSTAFAR is distinct from token-eviction schemes like <a href="https://arxiv.org/abs/2306.14048">H2O</a>
                because the sparsity it produces is "unstructured". H2O evicts entire tokens, and its compressibility
                stems from the fact that $Attn(Q, K, V)$ does not care about token ordering. So, instead of zeroing-out
                rows, we just remove the entire row and get a smaller matrix.
            </p>
            <p>
                But with MUSTAFAR, we have no choice but to deal with a sparse matrix, which has to be compressed on the
                basis of its low-entropy content, not its smaller size.
            </p>
            <p>
                Sparse matrices come up in other fields as well. Model <u>weight pruning</u> is the most common example.
            </p>
            <p>
                To actually exploit these pruning techniques, we need an efficient scheme for compressing
                unstructured, sparse data. The compression scheme should be fast, well suited for parallel,
                vector-processors, and provide
                high compression ratios.
            </p>

            <h2>Existing Compression Schemes</h2>
            <p>
                Existing schemes for compressing unstructured, sparse data are usually bitmaps or CSR/CSC/COO. This
                class of compression schemes is defined by storing the data in multiple structures: a dense array
                storing the non-zero values and some metadata storing the location of zeroes (<a
                    href="https://en.wikipedia.org/wiki/Sparse_matrix#Storage">see this for more</a>).
            </p>
            <p>
                The MUSTAFAR paper <a href="https://github.com/dhjoo98/mustafar/blob/main/kernel/compression.py">builds
                    Triton kernels</a> for bitmap based sparse KV cache compression, but their implementation reveals
                the limitation of the "data + metadata" idea:
            </p>
            <ul>
                <li>They are challenging to parallelize</li>
                <li>They operate in multiple distinct steps or need multiple kernels</li>
                <li>They need non-trivial global operations like prefix-sums</li>
                <li>They issue inefficient, irregular, uncoalescable memory read patterns</li>
                <li>In general, they have low arithmetic intensity</li>
            </ul>
            <p>
                In addition, "data + metadata" schemes can be <u>information-theoretically suboptimal</u>. Bitmaps for
                example have a strict 1-bit-per-symbol ceiling, even if the true entropy is $\lt 1$.
            </p>
            <p>
                Some AI hardware units have direct support for sparse data loading. Apple's Neural Engine <a
                    href="https://apple.github.io/coremltools/docs-guides/source/opt-pruning-perf.html">seems to support
                    RLE</a>, but that is only effective if the zeroes are in continuous groups. If the zeroes are
                uniformly spread, then RLE is ineffective, even though the actual entropy content doesn't care about the
                location of zeroes.
            </p>
            <p>
                NVIDIA GPUs have support for <a
                    href="https://developer.nvidia.com/blog/structured-sparsity-in-the-nvidia-ampere-architecture-and-applications-in-search-engines/">2:4
                    sparsity</a> but that is explicitly a scheme for structured sparsity only. The $0$s must follow a
                2:4 pattern, but pruning techniques make no guarantee that they will produce such a pattern.
            </p>
            <p>
                Based on the limitations of existing schemes, let's try and devise one with the following properties:
            </p>
            <ul>
                <li>Be close to information-theoretically optimal</li>
                <li>Should not use complex, global operations like prefix-sum</li>
                <li>Should only have regular, coalescable memory reads</li>
                <li>Should not have long, divergent branching</li>
            </ul>

            <h2>Asymmetric Numeral Systems</h2>
            <p>
                From these requirements, let's consider <a
                    href="https://en.wikipedia.org/wiki/Asymmetric_numeral_systems">Asymmetric Numeral Systems
                    (ANS)</a>. ANS is an <a href="https://en.wikipedia.org/wiki/Entropy_coding">entropy coding</a>
                algorithm, forming the backbone of Facebook's widely used compression tool <a
                    href="https://en.wikipedia.org/wiki/Zstd">zstd</a>.
            </p>
            <div class="callout aside">
                <strong>Resources</strong>
                <p>
                    A full explanation of ANS is out of scope of this article. I refer the reader to:
                </p>
                <ol>
                    <li><a
                            href="https://kedartatwawadi.github.io/post--ANS/">https://kedartatwawadi.github.io/post--ANS/</a>
                    </li>
                    <li><a
                            href="https://fastcompression.blogspot.com/2013/12/finite-state-entropy-new-breed-of.html">https://fastcompression.blogspot.com/2013/12/finite-state-entropy-new-breed-of.html</a>
                    </li>
                    <li>TODO: more</li>
                </ol>
                <p>
                    Its worth mentioning that the second link is Yann Collet's blog, the creator of <code>zstd</code>.
                </p>
            </div>

            <p>
                More specifically, let's consider the <u>Range ANS or rANS</u> variant of the algorithm (the other
                being Table ANS or tANS).
            </p>
            <p>
                rANS is a simple, pure arithmetic compression algorithm. All it needs to compress a given data stream
                are the <u>frequency tables</u> describing the probability distribution of each possible symbol in the
                data.
            </p>
            <p>
                rANS conceptually involves scanning through a <u>data-stream</u> one symbol at a time, in-order. As we
                walk through the data-stream, we accumulate a compressed representation of each symbol into an integer
                <u>state variable</u>.
            </p>
            <p>
                Then, for decompression, we perform the procedure in reverse, obtaining decompressed symbols back from
                the state variable.
            </p>
            <p>
                The core of the algorithm is that each symbol, depending on its frequency, is assigned a <u>range</u> or
                a <u>slot</u> in the state variable. Frequent symbols have large ranges (and consume fewer bits) and
                infrequent symbols have small ranges (and consume more bits). The frequency tables thus have the role
                of <u>allocating bits to symbols</u>.
            </p>
            <h4>Compression Procedure</h4>
            $$ X_t = \left\lfloor \frac{X_{t-1}}{\text{freq}[s_t]} \right\rfloor * M + \text{cfreq}[s_t] +
            mod(X_{t-1}, \text{freq}[s_t]) $$

            <br>
            <h4>Decompression Procedure</h4>
            $$
            slot = mod(X_t, M) \\[2ex]
            s_t = \text{inv\_cfreq}[slot] \\[2ex]
            X_{t-1} = \left\lfloor \frac{X_t}{M} \right\rfloor * \text{freq}[s_t] + slot - \text{cfreq}[s_t]
            $$

            Where:
            <ul>
                <li>$s_t$ is the symbol we encountered at time $t$</li>
                <li>$X_t$ is the state variable at time $t$</li>
                <li>$\text{freq}[s_t]$ is the frequency table, telling us expected frequency of symbol $s_t$</li>
                <li>$\text{cfreq}[s_t]$ is the cumulative frequency table</li>
                <li>$\text{inv\_cfreq}[slot]$ is a table mapping a slot back to the symbol that can fall in this range
                </li>
                <li>$M = \sum\limits_{s} \text{freq}[s]$ such that the probability of each symbol is $P_s =
                    \text{freq}[s] \ / \ M$</li>
            </ul>
            <p>
                rANS is a potent scheme for compressing sparse data. It is mostly arithmetic, can be parallelized, can
                match the data-stream's entropy exactly and doesn't separate data and metadata. Everything is stored in
                the integer state variable.
            </p>
            <p>
                But vanilla rANS has one performance limitation when we implement it on vector processors: <u>the three
                    table reads are uncoalescable</u>. Consider a set of threads each processing a data-stream. At a
                given time-step, if each thread processes a different symbol, then they issue entirely independent
                read-requests to the tables. This causes <u>bank-conflicts</u> and <u>serialization</u>.
            </p>
            <p>So although rANS is efficient, we can do even better if we get rid of the frequency table lookups.</p>
            <div class="callout aside">
                <strong>Resources (Again)</strong>
                <p>
                    I emphasize again: this article is <i>not</i> the right place to learn about ANS for the first time.
                    I only aim to give a "feel" about what the algorithm looks like. Refer to other resources because
                    there are several concepts (such as renormalization) that I will not mention at all, but are crucial
                    for a practical implementation.
                </p>
            </div>

            <h2>Resiliency and Probability Models</h2>
            <p>
                A very crucial property of ANS compression is that the frequency tables are only used for allocating
                bits to symbols. Even if the real data-stream doesn't match the frequency tables at all, the algorithm
                still works, you just get inferior compression ratio (maybe even $\lt 1$, i.e. expansion).
            </p>
            <p>
                This means that <u>rANS is resilient</u>: it can tolerate divergence between the real data and the
                frequency tables. Compression still works well even if the frequency tables only approximate the real
                symbol distribution.
            </p>
            <p>
                This means that we can <u>replace the frequency tables</u> with <u>probability models</u>, i.e. some
                mathematical formula that tells us the expected frequency of each symbol. If we can derive a probability
                model, then the rANS code <u>does not need any table-lookups</u>. The tables can be <u>replaced by
                    computed functions</u>.
            </p>
            <figure>
                <img src="img/ssrans/gaussian_diagram.png" title="Example Gaussian Probability Model"
                    alt="Example of Gaussian Probability Model">
                <figcaption>
                    Figure 1: Example of a Gaussian probability model and the real data it represents. The distributions
                    don't match, yet rANS works losslessly and decent compression gains are achieved.
                </figcaption>
            </figure>
            <p>
                In the figure above, even though the real data only has an approximately Gaussian structure, we can
                still use a Gaussian probability model. And even though this is not optimal (optimal model would match
                the data exactly), we have <i>at least</i> exploited the Gaussian structure in the data.
            </p>
            <p>
                In this example, we don't need to use table lookups at all. We can directly use the Gaussian formula,
                parametrized by $\mu$ and $\sigma^2$, to <i>compute</i> the symbol frequencies.
            </p>
            <p>
                This creates a very lucrative optimization for rANS: If we can replace table lookups with computed
                functions from a probability model, we have eliminated a significant source of irregular memory
                accesses. This helps us <u>satisfy all 4 requirements</u> I listed in the previous section.
            </p>

            <div class="callout aside">
                <strong>Resiliency Intuition: Part 1</strong>
                <p>It is <i>crucial</i> to understand this intuitively, so here's an example:</p>
                <p>
                    Let's say our data-stream's true distribution is a sum of a Gaussian and a Laplacian. These are two
                    <u>structural facts</u> about our data that we can exploit for compression. If we use the Gaussian
                    as the algorithm's probability model, then we have exploited the Gaussian fact, and left the
                    Laplacian on the table. Likewise, if we use Laplacian as the model, we've exploited that and left
                    the Gaussian fact on the table.
                </p>
                <p>
                    The idea is that if our data has multiple structural facts, we can <u>pick and choose</u> which ones
                    we include in the probability model.
                </p>
                <p>
                    This is relevant to us because we will now create a probability model for sparse data. Even if the
                    real data has multiple structural facts, we know that sparsity is one of them. So, all we need to do
                    is to <u>model sparsity</u> and we will <i>still</i> enjoy considerable compression, even if there
                    are other facts we have left on the table.
                </p>
                <p>This is why ANS resiliency is crucial to us.</p>
            </div>

            <h2>Probability Model for Sparse Data</h2>
            <p>
                We need to first define what exactly we mean by sparsity in an information-theoretic manner. First of
                all, let's call the zero-symbol " $\varnothing$ " to avoid confusion with the value zero.
            <div class="callout conclusion">
                <strong>Definition: Sparse Data</strong>
                Data is said to be sparse if it has a "disproportionately high" number of $\varnothing$s
            </div>
            </p>

            <p>
                To define what we mean by "disproportionately high", we can build two <u>archetypical models</u> of
                dense and sparse data:
            </p>
            <div class="callout conclusion">
                <strong>Definition: Archetypical Models</strong>
                <p>Let's define what the symbol histogram looks like for archetypical sparse and dense data.</p>
                <p>
                    Define $F(\varnothing)$ as the frequency of the $\varnothing$ symbol, and
                    $F(\text{non-}\varnothing)$ as the frequency of <i>any</i> other symbol.
                </p>
                <p>
                    In archetypical dense data, $F(\varnothing) = F(\text{non-}\varnothing)$ and the data is
                    <u>uniformly distributed</u>.
                </p>
                <p>
                    In archetypical sparse data, $F(\varnothing) \gt F(\text{non-}\varnothing)$ and the data is
                    <u>non-uniformly distributed</u>.
                </p>
                <figure>
                    <img src="img/ssrans/histogram_archetype.png" title="Archetypical Histograms"
                        alt="Archetypical Histograms">
                    <figcaption>
                        Figure 2: Archetypical symbol probability distributions for dense and sparse data. Dense is
                        completely uniform and sparse assumes a higher number of $\varnothing$s.
                    </figcaption>
                </figure>
                <p>
                    Similar to the Gaussian example we took before, the idea is that dense data is "roughly" like the
                    left figure, and sparse data is like the right.
                </p>
            </div>

            <p>
                This sparse archetype will be our probability model. We just need to calculate values for
                $F(\varnothing)$ and $F(\text{non-}\varnothing)$.
            </p>
            <figure>
                <img src="img/ssrans/sparse_archetype.png" title="Example of Sparse Probability Model"
                    alt="Example of Sparse Probability Model">
                <figcaption>
                    Figure 3: Example of our sparse probability model and the real data it represents.
                </figcaption>
            </figure>
            <div class="callout aside">
                <strong>Resiliency Intuition: Part 2</strong>
                <p>
                    As discussed before, the only structural fact we have modelled is sparsity. We have modelled the
                    difference between $\varnothing$ and $\text{non-}\varnothing$ symbols, assuming data is otherwise
                    uniform. Each $\text{non-}\varnothing$ symbol has the same frequency: $F(\text{non-}\varnothing)$.
                </p>
                <p>
                    There could be structure within the $\text{non-}\varnothing$ symbols too, but we don't care about
                    that. We have picked and chosen only the fact of sparsity to exploit.
                </p>
            </div>

            <p>A few more definitions pertaining to the above histograms:</p>
            <div class="callout conclusion">
                <strong>Definition: $N$ and $M$</strong>
                <p>
                    Define $N$ as the number of possible symbols (i.e. number of bars in the histogram). So for 8-bit
                    data, $N = 256$.
                </p>
                <p>
                    And define $M = \sum\limits_{s} \text{freq}[s]$, (i.e. sum of bar heights).
                </p>
                <p>Note that $M$ has the same definition here as in the rANS formula discussed before.</p>
                <p>It can be seen that:</p>
                $$ M = F(\varnothing) + (N-1) F(\text{non-}\varnothing) $$
            </div>

            <p>Now we can define a measure of sparsity:</p>
            <div class="callout conclusion">
                <strong>Definition: Sparsity</strong>
                <p>
                    Define sparsity $S$ as the ratio of probability of finding a $\varnothing$ symbol vs finding a
                    $\text{non-}\varnothing$ symbol:
                </p>
                $$ S = \frac{F(\varnothing)}{(N-1) F(\text{non-}\varnothing)} $$
            </div>

            <p>Using the above formulas for $M$ and $S$, we can solve to get:</p>
            $$
            F(\varnothing) = M \left( \frac{S}{S+1} \right) \\[3ex]
            F(\text{non-}\varnothing) = \frac{M}{(N-1)(S+1)}
            $$

            <strong>This completes the probability model for sparse data.</strong>
            <p>Given $M$, $N$ and $S$, we now have formulas to calculate the frequency of each symbol.</p>

            <div class="callout aside">
                <strong>Percentage Sparsity</strong>
                <p>
                    Note that the above definition of $S$ is a probabilistic definition. This suits our needs but
                    colloquially, sparsity is written as the percentage of $\varnothing$ symbols:
                    $$ S_{\text{percent}} = \frac{F(\varnothing)}{M} \times 100 \% $$
                </p>
                <p>I will use "percentage-sparsity" to refer to this definition.</p>
            </div>

            <h2>Sparsity-Specialized rANS</h2>
            <p>
                We can now use the above probability model to replace the rANS tables:
                $$
                \begin{aligned}
                \text{freq}[s_t] &= \begin{cases}
                F(\varnothing) & \text{if } s_t = \varnothing \\
                F(\text{non-}\varnothing) & \text{otherwise}
                \end{cases} \\[4ex]

                \text{cfreq}[s_t] &= \begin{cases}
                0 & \text{if } s_t = \varnothing \\
                F(\varnothing) + (s_t - 1) \cdot F(\text{non-}\varnothing) & \text{otherwise}
                \end{cases} \\[4ex]

                \text{inv\_cfreq}[\text{slot}] &= \begin{cases}
                \varnothing & \text{if } \text{slot} \lt F(\varnothing) \\ 1 + \left\lfloor \frac{\text{slot} -
                F(\varnothing)}{F(\text{non-}\varnothing)} \right\rfloor & \text{otherwise} \end{cases}
                \end{aligned}
                $$
            </p>

            <p>
                Integrating this into the rANS compression and decompression procedures yields our
                <u>Sparsity-Specialized rANS (SSrANS)</u> algorithm, i.e. rANS with a probability model for sparse data
                instead of table-lookups.
            </p>
            <p>
                Here is a code example of SSrANS:
            <pre><code class="language-python"># Setup Inputs
M = ...
N = ...
S = ...

F_ZERO = (M * S) // (S + 1)
F_NON_ZERO = M // ((N - 1) * (S + 1))


# Probability model functions
def freq(s):
    if s == 0:
        return F_ZERO
    else:
        return F_NON_ZERO

def cfreq(s):
    if s == 0:
        return 0
    else:
        return F_ZERO + (s - 1) * F_NON_ZERO

def inv_cfreq(slot):
    if slot < F_ZERO:
        return 0
    else:
      return 1 + (slot - F_ZERO) // F_NON_ZERO


# Main procedures
def compress(x, s):
    return (x // freq(s)) * M + cfreq(s) + x % freq(s)

def decompress(x):
    slot = x % M
    s = inv_cfreq(slot)
    new_x = (x // M) * freq(s) + slot - cfreq(s)
    return s, new_x</code></pre>
            </p>

            We can see that this satisfies our 4 requirements:
            <ul>
                <li>It optimally models sparsity without a 1 bit-per-symbol ceiling (unlike bitmaps)</li>
                <li>When vectorized, each thread runs these procedures independently with no global
                    operations like prefix-sum or inter-thread communication</li>
                <li>It has only one memory read (when reading the data-stream in compression, or the state variables in
                    decompression). This read can be coalesced across threads.</li>
                <li>It has only short branches that can be implemented using predicated instructions or masks</li>
            </ul>

            <h2>Constant Rounding</h2>
            <p>
                rANS only operates on integers. All values involved <i>must</i> be integers. This presents a problem
                because the two constants we calculate, $F(\varnothing)$ and $F(\text{non-}\varnothing)$, may not
                necessarily be integers. To solve this we need to <u>round them to the nearest integer</u>, while
                ensuring that the $M = F(\varnothing) + (N-1) F(\text{non-}\varnothing)$ condition still holds. If the
                rounding error is small enough, then the impact on optimality is negligible.
            </p>
            <p>
                One simple rounding strategy is:
            <pre><code class="language-python">def round_ssrans_params(M, N, S):
    # Step 1: Round F_nonzero to nearest integer, minimum 1
    F_nonzero = max(1, round(M / ((N - 1) * (S + 1))))

    # Step 2: Derive F_zero from the constraint M = F_zero + (N-1) * F_nonzero
    F_zero = M - (N - 1) * F_nonzero

    # Step 3: If F_zero is too small, fix it and recompute
    if F_zero < 1:
        F_zero = 1
        F_nonzero = (M - F_zero) // (N - 1)
        F_zero = M - (N - 1) * F_nonzero

    # Both guaranteed to be integers
    return F_zero, F_nonzero
</code></pre>
            <div class="callout aside">
                <strong>Choosing $M$</strong>
                <p>
                    Note that unlike $N$ and $S$, $M$ isn't a property of the data-stream. Rather, its a constant we
                    have to set. $M$ intuitively controls the "resolution" or "granularity" of the probability model.
                </p>
                <p>
                    $M$ is adjustable (instead of being a constant function of the data-stream) because in all
                    entropy-coders, we only care about the relative distribution of symbol frequencies, not their
                    absolute values. So we can scale the histogram however we want, and therefore $M$ is not fixed.
                </p>
                <p>
                    For efficiency, we commonly set $M$ to a power-of-two.
                </p>
            </div>
            <div class="callout aside">
                <strong>Is $S$ an input?</strong>
                <p>
                    In this article, I treat the probability model as being parametrized by $(M, N, S)$. This means both
                    compression and decompression requires us to know these three constants. This is fine for $M$ and
                    $N$, but is a little unusual for $S$, because how can we know the sparsity of the data-stream before
                    we've processed it?
                </p>
                <p>
                    For our particular motivation, KV cache pruning, this is acceptable, since pruning typically
                    assumes a target percentage-sparsity to begin with, i.e. we already know how much pruning we're
                    going to do. Hence, we already know $S$ before compressing/decompressing the pruned KV cache.
                </P>
                <p>
                    In case we don't know $S$ as a constant upfront, we can build a dynamic model, where $S$ is "learnt"
                    as we process the data-stream.
                </p>
            </div>
            </p>

            <h2>Vectorization</h2>
            <p>
                rANS is inherently a sequential algorithm. However parallel variants exist, such as <a
                    href="https://dl.acm.org/doi/10.1145/3337821.3337888">Massively Parallel ANS Decoding on GPUs</a>,
                as well as practical CUDA implementations, most notably <a
                    href="https://github.com/facebookresearch/dietgpu/tree/main">dietgpu</a>.
            </p>
            <p>
                In general, parallelized rANS relies on dividing the data into <u>multiple, interleaved streams</u>,
                instead of a single data-stream. Now each thread can independently compress/decompress each stream.
            </p>

            <h2>Experimental Results</h2>
            <p>
                To show the performance advantages of SSrANS, let's compare it against a regular rANS implementation as
                well as a bitmap scheme for sparse data. I will implement all 3 as CUDA kernels and test them on my
                NVIDIA RTX 5070.
            </p>
            <p>
                The CUDA kernels and benchmarking code <a
                    href="https://github.com/UditDey/pitch_site/blob/main/code/ssrans/benchmark">can be found here</a>.
            </p>
            <p>
                In order to measure just the algorithms' core performance. The kernels only test the raw throughput of
                the core decompression loops, and therefore:
            </p>
            <ul>
                <li>They operate on synthetic data</li>
                <li>Only read from shared-memory</li>
                <li>Operate in a single block on a single streaming-multiprocessor</li>
            </ul>
            <p>The kernels only "go through the motions" of the algorithms.</p>
            <figure>
                <img title="Throughput Comparison of the 3 Algorithms">
                <figcaption>Figure 4: Throughput of the algorithm kernels measured in symbols-per-second</figcaption>
            </figure>
            <p>
                SSrANS has <u>55% higher throughput</u> than vanilla rANS, and XX% higher throughput than bitmaps. Its
                clear that bitmaps are the worst option for parallel, vectorized implementations, and
                sparsity-specialization provides considerable gains to vanilla rANS.
            </p>
            <p>
                The SSrANS and vanilla rANS CUDA kernels look almost identical, but the difference can be seen clearly
                when comparing the <a href="https://github.com/UditDey/pitch_site/blob/main/code/ssrans/ptx">kernels'
                    PTX code</a>. The SSrANS kernel is significantly smaller because in addition to removing the three
                <code>ld.shared.u32</code> instructions, we've also gotten rid of the address calculations. SSrANS has a
                clear performance advantage even aside from the bank-conflict and uncoalescable reads problem.
            </p>
            <p>
                Finally, we need to also compare compression ratios. For this, we have <a
                    href="https://github.com/UditDey/pitch_site/blob/main/code/ssrans/ratio"
                    style="white-space: nowrap">a python script</a> that compresses random data with adjustable
                sparsity, and compares the compression ratios of bitmaps, rANS (with perfect frequency tables) and
                SSrANS:
            </p>
            <figure>
                <img src="img/ssrans/ssrans_compression_ratio.png" title="Compression Ratio Comparison"
                    alt="Compression Ratio Comparison">
                <figcaption>
                    Figure 5: Comparison of the same data being compressed under SSrANS, vanilla rANS and bitmaps.
                </figcaption>
            </figure>
            <p>
                At high percentage-sparsity, bitmaps deviate from the optimal Shannon entropy, while rANS and SSrANS
                stay close to it. It should also be noted that vanilla rANS and SSrANS compression ratios are so similar
                that the lines are indistinguishable. This is notable since vanilla rANS has exact frequency tables, but
                SSrANS only has an approximate probability model.
            </p>

            <h2>Conclusion</h2>
            <p>
                In this article, we started with a discussion of KV cache pruning, particularly MUSTAFAR, and showed
                that existing schemes for compressing unstructured sparse data have efficiency limitations.
            </p>
            <p>
                Based on that, we developed <b>Sparsity-Specialized rANS (SSrANS)</b>, a variation of the rANS
                compression algorithm where a probability model for sparse data is used to replace the table lookups. We
                experimentally showed that SSrANS has superior performance and compression ratios than bitmap based
                schemes.
            </p>
            <p>
                Overall, unstructured sparsity is a useful tool for improving LLM memory footprint and overall
                efficiency, and I hope that algorithms like SSrANS allows us to exploit it in a practical and performant
                manner.
            </p>
        </div>
    </main>

    <footer>
        <div class="container">
            <p><a href="mailto:your.email@example.com">your.email@example.com</a> · <a
                    href="https://linkedin.com/in/yourprofile">LinkedIn</a></p>
        </div>
    </footer>
</body>

</html>